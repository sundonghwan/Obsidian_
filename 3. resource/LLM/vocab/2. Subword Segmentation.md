---
cssclasses:
  - width-100
aliases:
---
## 1. 개요
---
-  Vocab을 만드는 방식을 조사하다 단어 분리 기능을 따로 이해할 필요가 있을 것 같아 기록하게 되었습니다.
- Subword Segmentation의 종류는 아래와 같습니다
	- BPE (Byte Pair Encoding)
	- WordPiece Tokenizer
	- Unigram Language Model Tokenizer


## 2. BPE [참고자료](https://wikidocs.net/22592)
---
- BPE (Byte Pari Encoding)이란 ?
	- 자연어 처리에서의 BPE는 서브워드 분리(subword segmentation) 알고리즘으로 글자 단위에서 점차적으로 단어 집합(Vocabulary)을 만들어내는 Bottom up 방식
	- 우선 훈련 데이터에 있는 단어들을 모든 글자(chracters)또는 유니코드(unicode)단위로 단어 집합(vocabulary)를 만들고 가장 많이 등장하는 유니그램[^1]을 하나의 유니그램으로 통합
- 동작 방식 예제
	- 어떤 훈련 데이터로부터 각 단어들의 빈도수를 카운트했다고 해보겠습니다. 그리고 각 단어와 각 단어의 빈도수가 기록되어져 있는 해당 결과는 임의로 딕셔너리(dictionary)란 이름을 붙였음.

```
	# dictionary
	# 훈련데이터에 있는 단어와 등장 빈도수
	low: 5, lower: 2, newest: 6, widest: 3
```

- 해당 데이터셋을 활용해서 단어 집합을 얻는 것은 간단합니다.
```
# vocabulary
low,lower,newest, widset
```
- 단어 집합은 중복을 배제한 단어들의 집합을 의미하므로 기존에 배운 단어 집합의 정의라면, 이 훈련 데이터의 단어 집합에는 'low', 'lower', 'newest', 'widest'라는 4개의 단어가 존재합니다.
- 그리고 이 경우 테스트 과정에서 'lowest'란 단어가 등장한다면 기계는 이 단어를 학습한 적이 없으므로 해당 단어에 대해서 제대로 대응하지 못하는 OOV 문제가 발생합니다

- 다음은 BPE 알고리즘을 적용한 경우입니다. 
	- Chracter 단위로 분리
```
# dictionary
l o w : 5, l o w e r : 2, n e w e s t : 6, w i d e s t : 3
```

- 딕셔너리를 참고로 한 초기 단어 집합(vocabulary)는 아래와 같습니다. 아래 예제는 글자 단위로 간단히 분리된 상태입니다
```
l, o, w, e, r, n, s, t, i, d
```

- BPE의 특징은 알고리즘의 동작을 몇 회 반복(iteration)할 것인지를 사용자가 정한다는 점입니다. 여기서는 총 10회를 수행한다고 가정합니다. 다시 말해 **가장 빈도수가 높은 유니그램의 쌍을 하나의 유니그램으로 통합**하는 과정을 총 10회 반복합니다. 위의 딕셔너리에 따르면 빈도수가 현재 가장 높은 유니그램의 쌍은 (e, s)입니다.

- **1회 - 딕셔너리를 참고로 하였을 때 빈도수가 9로 가장 높은 (e, s)의 쌍을 es로 통합합니다.**

```yaml
# dictionary update!
l o w : 5,
l o w e r : 2,
n e w es t : 6,
w i d es t : 3
```

```bash
# vocabulary update!
l, o, w, e, r, n, s, t, i, d, es
```

- **2회 - 빈도수가 9로 가장 높은 (es, t)의 쌍을 est로 통합합니다.**

```yaml
# dictionary update!
l o w : 5,
l o w e r : 2,
n e w est : 6,
w i d est : 3
```

```bash
# vocabulary update!
l, o, w, e, r, n, s, t, i, d, es, est
```

- **3회 - 빈도수가 7로 가장 높은 (l, o)의 쌍을 lo로 통합합니다.**

```yaml
# dictionary update!
lo w : 5,
lo w e r : 2,
n e w est : 6,
w i d est : 3
```

```bash
# vocabulary update!
l, o, w, e, r, n, s, t, i, d, es, est, lo
```

- 이와 같은 방식으로 총 10회 반복하였을 때 얻은 딕셔너리와 단어 집합은 아래와 같습니다.

```yaml
# dictionary update!
low : 5,
low e r : 2,
newest : 6,
widest : 3
```

```cpp
# vocabulary update!
l, o, w, e, r, n, s, t, i, d, es, est, lo, low, ne, new, newest, wi, wid, widest
```

- 이 경우 테스트 과정에서 'lowest'란 단어가 등장한다면, 기존에는 OOV에 해당되는 단어가 되었겠지만 BPE 알고리즘을 사용한 위의 단어 집합에서는 더 이상 'lowest'는 OOV가 아닙니다. 기계는 우선 'lowest'를 전부 글자 단위로 분할합니다. 즉, 'l, o, w, e, s, t'가 됩니다. 그리고 기계는 위의 단어 집합을 참고로 하여 'low'와 'est'를 찾아냅니다. 즉, 'lowest'를 기계는 'low'와 'est' 두 단어로 인코딩합니다. 그리고 이 두 단어는 둘 다 단어 집합에 있는 단어이므로 OOV가 아닙니다.

- 이 동작 과정을 그림으로 표현한다면 다음과 같습니다.
 ![[Pasted image 20240514180417.png]]
## 각주

[^1]: 유니그램: 토큰 1개로 이루어진 연속된 시퀀스

